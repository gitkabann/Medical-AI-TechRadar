# app/workers/agents.py
import asyncio
from app.core.base_worker import BaseWorker
from app.core.event_bus import bus, Topic
from app.models.protocol import TaskPayload
from app.tools.pubmed_client import ingest_pubmed
from app.tools.arxiv_client import ingest_arxiv
from app.tools.github_client import ingest_github
from app.tools.trials_client import ingest_trials
from app.tools.rag_query import query_rag
from app.agents.writer import generate_markdown_report
from app.tools.pdf_exporter import save_markdown_as_pdf
from app.core.state_manager import state_manager
from app.core.memory import task_memory
from app.models.plan import ExecutionPlan 

# 1. Planner Agent: é€‰æ‹©å’Œå†³å®šä»»åŠ¡çš„ä¸‹ä¸€ä¸ª Agent
class PlannerAgent(BaseWorker):
    def __init__(self):
        super().__init__(Topic.PLANNER, Topic.CRAWLER)

    def process(self, payload: TaskPayload) -> TaskPayload:
        topic = payload.topic
        depth = payload.params.get("depth", "light")
        print(f"ğŸ§  [Planner] æ”¶åˆ°ä»»åŠ¡: {topic} | æ¨¡å¼: {depth}")
        # åˆå§‹åŒ–ä»»åŠ¡è®°å½•
        state_manager.init_task(payload.task_id, payload.topic, payload.params)

        # è®°å¿†æ£€ç´¢
        # å°è¯•å›å¿†æ˜¯å¦åšè¿‡ç±»ä¼¼ä»»åŠ¡
        past_knowledge = task_memory.recall_task(topic)
        if past_knowledge:
            print(f"[Planner] å‘ç°ç±»ä¼¼ä»»åŠ¡è®°å¿†: {past_knowledge['topic']}")
            print("[Planner] ç­–ç•¥è°ƒæ•´: è·³è¿‡æŠ“å–ï¼Œå¤ç”¨å†å²çŸ¥è¯†ã€‚")
            # å°†å†å²æ•°æ®æ³¨å…¥ Payload
            payload.data["rag_context"] = [{
                "content": f"ã€å†å²çŸ¥è¯†å¤ç”¨ã€‘\nä¹‹å‰çš„ç ”ç©¶æ€»ç»“ï¼š{past_knowledge['summary']}",
                "metadata": {"source": "Memory", "type": "history"}
            }]
            # ç›´æ¥å‘å¸ƒåˆ° Writer
            next_payload = payload.next_step("memory_hit")
            bus.publish(Topic.WRITER, next_payload.model_dump())
            return None # é˜»æ­¢åç»­æµç¨‹
        # åˆ¶å®šè®¡åˆ’
        if depth == "deep":
            plan = ExecutionPlan.create_deep()
            print("[Planner] ç­–ç•¥: æ·±åº¦æ¨¡å¼ (æ–‡çŒ®+ä»£ç +è¯•éªŒ, Top-10)")
        else:
            plan = ExecutionPlan.create_light()
            print("[Planner] ç­–ç•¥: è½»é‡æ¨¡å¼ (æ–‡çŒ®+è¯•éªŒ, Top-3)")
        # å°†è®¡åˆ’æ³¨å…¥ Payload çš„ params ä¸­ï¼Œä¾›ä¸‹æ¸¸ä½¿ç”¨
        payload.params["execution_plan"] = plan.model_dump()
        print(f"ğŸ§  [Planner] æ‰§è¡Œè®¡åˆ’: {plan.model_dump()}")
        return payload.next_step("planning_done")

# 2. Crawler Agent: è´Ÿè´£å¹¶å‘æŠ“å–
class CrawlerAgent(BaseWorker):
    def __init__(self):
        super().__init__(Topic.CRAWLER, Topic.RAG)

    def process(self, payload: TaskPayload) -> TaskPayload:
        topic = payload.topic
        # è¯»å– Planner åˆ¶å®šå¥½çš„è®¡åˆ’
        plan_data = payload.params.get("execution_plan", ExecutionPlan.create_light().model_dump())
        plan = ExecutionPlan(**plan_data)
        print(f"ğŸ•·ï¸ [Crawler] æ‰§è¡Œè®¡åˆ’: {plan.sources} (Limit: {plan.max_items})")

        async def run_crawlers():
            tasks = []
            # åŠ¨æ€æ„å»º DAG (åŸºäº Plan)
            if "pubmed" in plan.sources:
                tasks.append(ingest_pubmed(topic, max_results=plan.max_items))
            if "arxiv" in plan.sources:
                tasks.append(ingest_arxiv(topic, max_results=plan.max_items))
            if "github" in plan.sources:
                tasks.append(ingest_github(topic, top_n=min(3, plan.max_items)))# GitHub æŠ“å–æ•°é‡ä¸å®œè¿‡å¤š
            

            # å¹¶å‘æ‰§è¡Œæ–‡çŒ®å’Œä»£ç æŠ“å–
            if tasks:
                await asyncio.gather(*tasks, return_exceptions=True)
            # è¯•éªŒæŠ“å– (å¦‚æœè®¡åˆ’å¯ç”¨)
            if plan.enable_trials:
                try:
                    ingest_trials(topic) # å‡è®¾ ingest_trials å†…éƒ¨å·²æ”¯æŒ limit æˆ–æš‚ä¸æ”¯æŒ
                    print("âœ… [Crawler] ClinicalTrials æŠ“å–å®Œæˆ")
                except Exception as e:
                    print(f"âš ï¸ [Crawler] Trials å¤±è´¥: {e}")
            else:
                print("â­ï¸ [Crawler] è·³è¿‡ ClinicalTrials (æ ¹æ®è®¡åˆ’)")

        asyncio.run(run_crawlers())
        
        return payload.next_step("crawling_done", {"plan_executed": plan.mode})

# 3. RAG Agent: è´Ÿè´£æ£€ç´¢
class RagAgent(BaseWorker):
    def __init__(self):
        super().__init__(Topic.RAG, Topic.WRITER)

    def process(self, payload: TaskPayload) -> TaskPayload:
        topic = payload.topic
        print(f"ğŸ” [RAG] æ­£åœ¨æ£€ç´¢ä¸Šä¸‹æ–‡...")
        
        results = query_rag(topic, top_k=5)
        # å°†ç»“æœå­˜å…¥ data ä¼ é€’ç»™ Writer
        # æ³¨æ„ï¼šresults æ˜¯ dict åˆ—è¡¨ï¼Œå¯ä»¥ç›´æ¥åºåˆ—åŒ–
        
        return payload.next_step("rag_done", {"rag_context": results})

# 4. Writer Agent: ç”ŸæˆæŠ¥å‘Š
class WriterAgent(BaseWorker):
    def __init__(self):
        super().__init__(Topic.WRITER, None) # é“¾æ¡ç»ˆç‚¹ï¼Œä¸å†å‘å¸ƒ

    def process(self, payload: TaskPayload) -> TaskPayload:
        topic = payload.topic
        context = payload.data.get("rag_context", [])
        
        print(f"âœï¸ [Writer] æ­£åœ¨æ’°å†™æŠ¥å‘Š...")
        report = generate_markdown_report(topic, context)
        
        # ä¿å­˜æ–‡ä»¶
        task_id = payload.task_id
        md_path = f"report_{task_id}.md"
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(report)
            
        # å¯¼å‡º PDF
        try:
            pdf_path = save_markdown_as_pdf(task_id, report)
            print(f"ğŸ‰ [Writer] ä»»åŠ¡å®Œæˆï¼PDF: {pdf_path}")
        except Exception:
            print("âš ï¸ PDF ç”Ÿæˆå¤±è´¥ï¼Œä½† MD å·²ä¿å­˜")

        # === å­˜å…¥è®°å¿† ===
        # æå–æŠ¥å‘Šçš„å‰ 500 å­—ä½œä¸ºæ‘˜è¦å­˜å…¥è®°å¿†åº“
        summary = report[:500].replace("#", "").replace("*", "")
        task_memory.remember_task(
            topic=topic,
            summary=summary,
            artifact_path=pdf_path
        )
        print(f"ğŸ§  [Writer] å·²å°†æœ¬ä»»åŠ¡å­˜å…¥é•¿æœŸè®°å¿†åº“ã€‚")
        # ==============================
        return None # ç»“æŸ