import httpx
from typing import List, Dict
from app.core.logger import get_logger
from app.tools.chunking import chunk_text
from app.models.document import DocumentChunk
from app.core.chaos import chaos  # å¯¼å…¥æ··æ²Œ

logger = get_logger(__name__)

async def fetch_arxiv(topic: str, max_results: int = 10) -> List[Dict]:
    # === åŸ‹é›· ===
    try:
        chaos.simulate("ArXiv_API") 
    except ConnectionError as e:
        print(f"âš ï¸ {e}")
        raise e 
    # ============
    """
    æœç´¢ arXiv æ–‡çŒ®ï¼Œè¿”å› title/abstract/date/url/doi ç­‰ä¿¡æ¯ã€‚
    arXiv ä½¿ç”¨ Atom XMLï¼Œéœ€è¦æ‰‹åŠ¨è§£æã€‚
    """
    try:
        ARXIV_API = "https://export.arxiv.org/api/query"
        headers = {
        "User-Agent": "Mozilla/5.0 (compatible; Agentic-RAG/1.0; +https://example.com)"
        }
        # æ„é€ æŸ¥è¯¢ URL
        params = {
            "search_query": f"all:{topic}",
            "start": 0,
            "max_results": max_results,
        }

        async with httpx.AsyncClient(timeout=10.0, headers=headers) as client:
            resp = await client.get(ARXIV_API, params=params)
            print(f"ğŸ“¡ [ArXiv] HTTP çŠ¶æ€ç : {resp.status_code}")
            if resp.status_code != 200:
                print(f"âŒ ArXiv è¿”å›é”™è¯¯çŠ¶æ€ç ã€‚å†…å®¹æ‘˜è¦: {resp.text[:200]}")
                return []
            xml_text = resp.text

            papers = parse_arxiv_xml(xml_text)
            logger.info(f"[arXiv] çŠ¶æ€ç ï¼š{resp.status_code}ï¼Œè§£æåˆ° {len(papers)} ç¯‡è®ºæ–‡ï¼ˆtopic='{topic}'ï¼‰")

            return papers
            
    except Exception as e:
        logger.error(f"ArXiv è¯·æ±‚å¤±è´¥: {e}")
        return []


def parse_arxiv_xml(xml_text: str) -> List[Dict]:
    """
    è§£æ arXiv API è¿”å›çš„ Atom XMLã€‚
    """
    import xml.etree.ElementTree as ET

    root = ET.fromstring(xml_text)
    ns = {"atom": "http://www.w3.org/2005/Atom"}

    results = []

    for entry in root.findall("atom:entry", ns):

        title = entry.find("atom:title", ns).text or ""
        print("Title:", title)
        abstract = entry.find("atom:summary", ns).text or ""

        date = entry.find("atom:published", ns).text or ""

        url = entry.find("atom:id", ns).text or ""

        # DOI æ˜¯å¯é€‰çš„
        doi_node = entry.find(".//atom:doi", ns)
        doi = doi_node.text if doi_node is not None else None

        results.append({
            "title": title.strip(),
            "abstract": abstract.strip(),
            "date": date,
            "url": url,
            "doi": doi,
        })

    return results


async def ingest_arxiv(topic: str) -> int:
    """arXiv â†’ åˆ†å— â†’ å…¥åº“"""
    papers = await fetch_arxiv(topic)
    all_chunks: List[DocumentChunk] = []

    for paper in papers:
        chunks = chunk_text(
            text=paper["abstract"],
            source="arxiv",
            metadata_extra=paper,
        )
        all_chunks.extend(chunks)

    from app.tools.chroma_client import ingest
    ingest(all_chunks)

    return len(all_chunks)
